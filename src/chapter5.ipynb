{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용자 id와 아이템 id만 사용해서 FM 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chapter5_modules import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = load_ratings()\n",
    "\n",
    "# User encoding\n",
    "user_dict = {} # user_id - index mapping\n",
    "for i in set(ratings['user_id']): # 중복제거, 순서대로 출력\n",
    "  user_dict[i] = len(user_dict)\n",
    "n_user = len(user_dict)\n",
    "\n",
    "# Item encoding\n",
    "item_dict = {}\n",
    "start_point = n_user\n",
    "for i in set(ratings['movie_id']):\n",
    "  item_dict[i] = start_point + len(item_dict)\n",
    "n_item = len(item_dict)\n",
    "\n",
    "num_x = n_user + n_item\n",
    "ratings = shuffle(ratings, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2625, 2625)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_x, len(user_dict)+len(item_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 0 cases...\n",
      "Encoding 10000 cases...\n",
      "Encoding 20000 cases...\n",
      "Encoding 30000 cases...\n",
      "Encoding 40000 cases...\n",
      "Encoding 50000 cases...\n",
      "Encoding 60000 cases...\n",
      "Encoding 70000 cases...\n",
      "Encoding 80000 cases...\n",
      "Encoding 90000 cases...\n"
     ]
    }
   ],
   "source": [
    "# Generate X data , sparse matrix -> coordinate format matrix \n",
    "w0 = np.mean(ratings['rating']) # global bias\n",
    "y = (ratings['rating'] - w0).values.tolist()\n",
    "\n",
    "data = []; \n",
    "\n",
    "for i in range(len(ratings)):\n",
    "  case = ratings.iloc[i]\n",
    "  \n",
    "  x_index = []; x_value = []\n",
    "  x_index.append(user_dict[case['user_id']]); x_value.append(1)\n",
    "  x_index.append(item_dict[case['movie_id']]); x_value.append(1)\n",
    "  \n",
    "  data.append([x_index, x_value])\n",
    "  \n",
    "  if (i % 10000) == 0:\n",
    "    print(f'Encoding {i} cases...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM:\n",
    "  def __init__(self, N:int, K:int, data:list, y, alpha:float, beta:float, \n",
    "               train_ratio:float=0.75, iterations:int=100, \n",
    "               tolerance:float=0.005, l2_reg:bool=True, verbose:bool=True):\n",
    "    \"\"\"FM inital function\n",
    "\n",
    "    Args:\n",
    "        N (int): # of x\n",
    "        K (int): # of latent feature\n",
    "        data (list): coo matrix\n",
    "        y (list): rating data\n",
    "        alpha (float): learnign rate\n",
    "        beta (float): regularization rate\n",
    "        train_ratio (float, optional): ratio of train set. Defaults to 0.75.\n",
    "        iterations (int, optional): # of learning. Defaults to 100.\n",
    "        tolerance (float, optional): 반복을 중단하는 RMSE의 기준. Defaults to 0.005.\n",
    "        l2_reg (bool, optional): 정규화를 할지 여부. Defaults to True.\n",
    "        verbose (bool, optional): 학습상황을 표시할지 여부. Defaults to True.\n",
    "    \"\"\"\n",
    "    self.K = K; self.N = N; self.n_cases = len(data)\n",
    "    self.alpha = alpha; self.beta = beta\n",
    "    self.iterations = iterations\n",
    "    self.tolerance = tolerance; self.l2_reg = l2_reg\n",
    "    self.verbose = verbose\n",
    "    # w 초기화\n",
    "    self.w = np.random.normal(scale=1./self.N, size=(self.N))\n",
    "    # v 초기화 (latent matrix)\n",
    "    self.v = np.random.normal(scale=1./self.K, size=(self.N, self.K))\n",
    "    # Train / Test 분리\n",
    "    cutoff = int(train_ratio * self.n_cases)\n",
    "    self.train_X = data[:cutoff]; self.test_X = data[cutoff:]\n",
    "    self.train_y = y[:cutoff]; self.test_y = y[cutoff:]\n",
    "  \n",
    "  def predict(self, x_idx:list, x_value:list) -> float:\n",
    "    \"\"\"x_idx와 x_value값으로 y_hat을 예측하는 함수\n",
    "\n",
    "    Args:\n",
    "        x_idx (list): x의 idx list\n",
    "        x_value (list): x의 value list\n",
    "\n",
    "    Returns:\n",
    "        float: y_hat\n",
    "    \"\"\"\n",
    "    \n",
    "    x_0 = np.array(x_value)\n",
    "    x_1 = x_0.reshape(-1, 1) # 2차원으로 변경 (vx와의 연산을 위함)\n",
    "    # cal bias score\n",
    "    bias_score = np.sum(self.w[x_idx] * x_0)\n",
    "    # cal latent score\n",
    "    vx = self.v[x_idx] * x_1\n",
    "    sum_vx = np.sum(vx, axis=0); sum_vx_2 = np.sum(vx * vx, axis=0)\n",
    "    \n",
    "    latent_score = 0.5 * np.sum(np.square(sum_vx) - sum_vx_2)\n",
    "    # cal prediction\n",
    "    y_hat = bias_score + latent_score\n",
    "    \n",
    "    return y_hat\n",
    "  \n",
    "  def sgd(self, X_data:list, y_data:list) -> float:\n",
    "    \"\"\"한번의 SGD를 진행하고 , RMSE값을 return 함.\n",
    "\n",
    "    Args:\n",
    "        X_data (list): input variables\n",
    "        y_data (list): rating data\n",
    "\n",
    "    Returns:\n",
    "        float: RMSE of before w, v\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    \n",
    "    for data, y in zip(X_data, y_data):\n",
    "      x_idx = data[0] # index\n",
    "      x_0 = np.array(data[1]) # value\n",
    "      x_1 = x_0.reshape(-1, 1) # 2차원으로 변경 (vx와의 연산을 위함)\n",
    "      vx = self.v[x_idx] * x_1\n",
    "      \n",
    "      y_hat = self.predict(x_idx, data[1])\n",
    "      y_pred.append(y_hat)\n",
    "      \n",
    "      error = y - y_hat\n",
    "      # update w, v\n",
    "      if self.l2_reg: \n",
    "        self.w[x_idx] += error * self.alpha * (x_0 - self.beta * self.w[x_idx])\n",
    "        self.v[x_idx] += error * self.alpha * (x_1 * sum(vx) - (vx * x_1) - self.beta * self.v[x_idx])\n",
    "      else:\n",
    "        self.w[x_idx] += error * self.alpha * x_0\n",
    "        self.v[x_idx] += error * self.alpha * (x_1 * sum(vx) - (vx * x_1))\n",
    "      \n",
    "    return RMSE(y_data, y_pred)\n",
    "    \n",
    "  def test(self)-> list[float]:\n",
    "    \"\"\"train하면서 RMSE를 계산하는 함수\n",
    "\n",
    "    Returns:\n",
    "        list[float]: iter별로 RMSE를 저장한 list \n",
    "    \"\"\"\n",
    "    # SGD를 iterations 숫자만큼 수행\n",
    "    best_RMSE = 10000; best_iteration = 0\n",
    "    training_process = []\n",
    "    \n",
    "    for i in range(self.iterations):\n",
    "      rmse1 = self.sgd(self.train_X, self.train_y)\n",
    "      rmse2 = self.test_rmse(self.test_X, self.test_y)\n",
    "      training_process.append((i, rmse1, rmse2))\n",
    "      if self.verbose and i+1 % 10 == 0:\n",
    "        print(f\"Iteration = {i+1} / Train RMSE = {rmse1:.6f} / Test RMSE = {rmse2:.6f}\")\n",
    "      if best_RMSE > rmse2:\n",
    "        best_RMSE = rmse2; best_iteration = i\n",
    "      elif (rmse2 - best_RMSE) >   self.tolerance: # rmse2가 tolerance 이상으로 best rmse보다 크다면 중단\n",
    "        break\n",
    "    \n",
    "    print(best_iteration, best_RMSE)\n",
    "    return training_process  \n",
    "    \n",
    "  def test_rmse(self, x_data, y_data) -> float:\n",
    "    \"\"\"현재 w와 v로 계산한 예측치의 RMSE를 계산하는 함수\n",
    "\n",
    "    Args:\n",
    "        x_data (list): _description_\n",
    "        y_data (list): _description_\n",
    "\n",
    "    Returns:\n",
    "        float: RMSE\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    \n",
    "    for data, y in zip(x_data, y_data):\n",
    "      y_hat = self.predict(data[0], data[1])\n",
    "      y_pred.append(y_hat)\n",
    "      \n",
    "    return RMSE(y_data, y_pred)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 0 cases...\n",
      "Encoding 10000 cases...\n",
      "Encoding 20000 cases...\n",
      "Encoding 30000 cases...\n",
      "Encoding 40000 cases...\n",
      "Encoding 50000 cases...\n",
      "Encoding 60000 cases...\n",
      "Encoding 70000 cases...\n",
      "Encoding 80000 cases...\n",
      "Encoding 90000 cases...\n",
      "Iteration = 10 / Train RMSE = 0.955977 / Test RMSE = 0.972691\n",
      "Iteration = 20 / Train RMSE = 0.934229 / Test RMSE = 0.957411\n",
      "Iteration = 30 / Train RMSE = 0.925384 / Test RMSE = 0.951438\n",
      "Iteration = 40 / Train RMSE = 0.920586 / Test RMSE = 0.948412\n",
      "Iteration = 50 / Train RMSE = 0.917455 / Test RMSE = 0.946656\n",
      "Iteration = 60 / Train RMSE = 0.914895 / Test RMSE = 0.945459\n",
      "Iteration = 70 / Train RMSE = 0.912038 / Test RMSE = 0.944364\n",
      "Iteration = 80 / Train RMSE = 0.907725 / Test RMSE = 0.942855\n",
      "Iteration = 90 / Train RMSE = 0.900192 / Test RMSE = 0.940212\n",
      "Iteration = 100 / Train RMSE = 0.887676 / Test RMSE = 0.935870\n",
      "Iteration = 110 / Train RMSE = 0.870094 / Test RMSE = 0.930350\n",
      "Iteration = 120 / Train RMSE = 0.848429 / Test RMSE = 0.924821\n",
      "Iteration = 130 / Train RMSE = 0.822634 / Test RMSE = 0.919831\n",
      "Iteration = 140 / Train RMSE = 0.792108 / Test RMSE = 0.915708\n",
      "Iteration = 150 / Train RMSE = 0.756803 / Test RMSE = 0.912875\n",
      "Iteration = 160 / Train RMSE = 0.717562 / Test RMSE = 0.911639\n",
      "Iteration = 170 / Train RMSE = 0.675712 / Test RMSE = 0.912055\n",
      "161 0.9115929242665275\n"
     ]
    }
   ],
   "source": [
    "from chapter5_modules import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data, y, num_x = get_data_y()\n",
    "\n",
    "K = 350\n",
    "\n",
    "FM_kwargs = {\n",
    "  'N': num_x,\n",
    "  'K': K,\n",
    "  'data': data,\n",
    "  'y': y,\n",
    "  'alpha': 0.0014,\n",
    "  'beta': 0.075,\n",
    "  'train_ratio': 0.75,\n",
    "  'iterations':600,\n",
    "  'tolerance': 0.0005,\n",
    "  'l2_reg':True,\n",
    "  'verbose': True\n",
    "}\n",
    "\n",
    "fm1 = FM(**FM_kwargs)\n",
    "result = fm1.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용자, 아이템 외에 추가 데이터까지 사용하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chapter5_modules import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, movies, ratings = get_dataset_1()\n",
    "\n",
    "# User encoding\n",
    "user_dict = {}\n",
    "for i in set(users['user_id']):\n",
    "  user_dict[i] = len(user_dict)\n",
    "n_user = len(user_dict)\n",
    "\n",
    "# Item encoding\n",
    "item_dict = {}\n",
    "start_point = n_user\n",
    "for i in set(movies['movie_id']):\n",
    "  item_dict[i] = start_point + len(item_dict)\n",
    "n_item = len(item_dict)\n",
    "start_point += n_item\n",
    "\n",
    "# Occupation encoding\n",
    "occ_dict = {}\n",
    "for i in set(users['occupation']):\n",
    "  occ_dict[i] = start_point + len(occ_dict)\n",
    "n_occ = len(occ_dict)\n",
    "start_point += n_occ\n",
    "\n",
    "# Gender encoding\n",
    "gender_dict = {}\n",
    "for i in set(users['sex']):\n",
    "  gender_dict[i] = start_point + len(gender_dict)\n",
    "n_gender = len(gender_dict)\n",
    "start_point += n_gender\n",
    "\n",
    "# Genre encoding\n",
    "genre_dict = {}\n",
    "genre = movies.columns.tolist()[5:]\n",
    "for i in genre:\n",
    "  genre_dict[i] = start_point + len(genre_dict)\n",
    "n_genre = len(genre_dict)\n",
    "start_point += n_genre\n",
    "\n",
    "# Age encoding\n",
    "age_index = start_point\n",
    "start_point += 1\n",
    "num_x = start_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, movies, ratings = get_dataset_1()\n",
    "\n",
    "# Merge data\n",
    "movies = movies.drop(['title', 'release_date', 'video release date', 'IMDB URL'], axis=1)\n",
    "users = users.drop(['zip_code'], axis=1)\n",
    "\n",
    "x = pd.merge(ratings, movies, how='outer', on='movie_id')\n",
    "x = pd.merge(x, users, how='outer', on='user_id')\n",
    "x = shuffle(x, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 0 cases...\n",
      "Encoding 10000 cases...\n",
      "Encoding 20000 cases...\n",
      "Encoding 30000 cases...\n",
      "Encoding 40000 cases...\n",
      "Encoding 50000 cases...\n",
      "Encoding 60000 cases...\n",
      "Encoding 70000 cases...\n",
      "Encoding 80000 cases...\n",
      "Encoding 90000 cases...\n"
     ]
    }
   ],
   "source": [
    "# Generate X data\n",
    "data = []; y = []\n",
    "\n",
    "age_mean = np.mean(x['age'])\n",
    "age_std = np.mean(x['age'])\n",
    "\n",
    "w0 = np.mean(x['rating'])\n",
    "y = (ratings['rating'] - w0).values.tolist()\n",
    "\n",
    "for i in range(len(x)):\n",
    "  case = x.iloc[i]\n",
    "  x_index = []; x_value = []\n",
    "  x_index.append(user_dict[case['user_id']]); x_value.append(1)\n",
    "  x_index.append(item_dict[case['movie_id']]); x_value.append(1)\n",
    "  x_index.append(occ_dict[case['occupation']]); x_value.append(1)\n",
    "  x_index.append(gender_dict[case['sex']]); x_value.append(1)\n",
    "  \n",
    "  for j in genre:\n",
    "    if case[j] == 1:\n",
    "      x_index.append(genre_dict[j]); x_value.append(1)\n",
    "      \n",
    "  x_index.append(age_index); x_value.append((case['age'] - age_mean) / age_std)\n",
    "  \n",
    "  data.append([x_index, x_value])\n",
    "\n",
    "  if (i % 10000) == 0:\n",
    "    print(f\"Encoding {i} cases...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 10 / Train RMSE = 1.115670 / Test RMSE = 1.130784\n",
      "Iteration = 20 / Train RMSE = 1.095844 / Test RMSE = 1.145900\n",
      "Iteration = 30 / Train RMSE = 1.067208 / Test RMSE = 1.166218\n",
      "0 1.1223694944531286\n"
     ]
    }
   ],
   "source": [
    "K = 100\n",
    "\n",
    "FM_kwargs = {\n",
    "  'N': num_x,\n",
    "  'K': K,\n",
    "  'data': data,\n",
    "  'y': y,\n",
    "  'alpha': 0.0014,\n",
    "  'beta': 0.075,\n",
    "  'train_ratio': 0.75,\n",
    "  'iterations':600,\n",
    "  'tolerance': 0.001,\n",
    "  'l2_reg':True,\n",
    "  'verbose': True\n",
    "}\n",
    "\n",
    "fm2 = FM(**FM_kwargs)\n",
    "result = fm2.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과가 더 안좋아짐. 파라미터 조정을 통해 성능을 향상시킬 수 있음.  \n",
    "여기서 알 수 있는 점은 변수를 무작정 추가한다고 반드시 좋은 결과가 나오는 것은 아니라는 점."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant_39",
   "language": "python",
   "name": "quant_39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
